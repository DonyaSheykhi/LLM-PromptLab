# Experiment Results — LLM-PromptLab

| Experiment            | Model               | Accuracy | F1-macro | Notes                  |
|-----------------------|---------------------|---------:|---------:|------------------------|
| GPT-4o 0-shot (demo)  | gpt-4o-mini         |   0.88   |   0.87   | reference expectation  |
| DistilBERT fine-tune  | distilbert-base     |   0.91   |   0.90   | trained on AG News     |
| Distilled student     | distilbert-base     |   0.89   |   0.88   | trained on LLM labels  |

